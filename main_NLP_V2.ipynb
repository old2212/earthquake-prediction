{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/olivier_ld/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dill as pickle\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import operator\n",
    "import re\n",
    "import gc\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "from wordcloud import STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import classification_report \n",
    "from torch.nn.functional import pad\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 µs, sys: 1 µs, total: 34 µs\n",
      "Wall time: 40.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clean(tweet): \n",
    "            \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
    "            \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
    "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
    "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
    "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
    "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
    "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
    "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
    "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
    "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
    "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
    "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
    "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
    "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
    "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
    "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
    "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
    "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
    "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
    "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
    "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
    "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
    "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
    "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
    "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
    "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
    "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
    "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
    "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
    "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
    "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
    "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
    "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
    "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
    "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
    "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
    "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
    "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
    "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
    "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
    "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
    "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
    "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
    "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
    "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
    "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
    "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
    "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
    "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
    "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
    "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
    "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
    "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
    "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
    "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
    "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
    "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
    "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
    "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
    "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
    "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
    "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
    "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
    "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
    "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
    "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
    "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
    "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
    "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
    "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
    "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
    "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
    "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
    "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
    "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
    "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
    "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
    "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
    "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
    "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
    "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
    "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
    "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
    "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
    "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
    "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
    "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
    "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
    "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
    "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
    "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
    "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
    "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
    "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
    "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
    "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
    "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
    "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
    "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
    "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
    "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
    "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
    "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
    "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
    "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
    "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
    "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
    "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
    "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
    "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
    "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
    "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
    "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
    "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
    "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
    "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
    "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
    "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
    "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
    "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
    "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
    "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
    "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
    "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
    "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
    "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
    "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
    "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
    "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
    "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
    "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
    "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
    "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
    "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
    "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
    "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
    "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
    "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
    "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
    "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
    "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
    "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
    "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
    "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
    "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
    "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
    "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
    "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
    "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
    "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
    "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
    "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
    "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
    "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
    "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
    "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
    "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
    "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
    "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
    "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
    "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
    "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
    "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
    "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
    "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
    "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
    "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
    "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
    "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
    "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
    "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
    "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
    "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
    "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
    "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
    "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
    "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
    "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
    "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
    "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
    "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
    "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
    "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
    "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
    "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
    "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
    "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
    "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
    "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
    "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
    "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
    "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
    "    tweet = re.sub(r\"Newss\", \"News\", tweet)\n",
    "    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n",
    "    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n",
    "    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n",
    "    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n",
    "    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n",
    "    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n",
    "    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n",
    "    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n",
    "    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n",
    "    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n",
    "    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n",
    "    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n",
    "    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n",
    "    tweet = re.sub(r\"andword\", \"and word\", tweet)\n",
    "    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n",
    "    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n",
    "    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n",
    "    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n",
    "    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n",
    "    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n",
    "    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n",
    "    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n",
    "    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n",
    "    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n",
    "    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n",
    "    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n",
    "    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n",
    "    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
    "    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n",
    "    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n",
    "    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n",
    "    tweet = re.sub(r\"evng\", \"evening\", tweet)\n",
    "    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n",
    "    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n",
    "    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n",
    "    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n",
    "    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n",
    "    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n",
    "    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n",
    "    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n",
    "    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n",
    "    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n",
    "    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n",
    "    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n",
    "    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n",
    "    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n",
    "    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n",
    "    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n",
    "    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n",
    "    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n",
    "    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n",
    "    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n",
    "    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n",
    "    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n",
    "    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n",
    "    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n",
    "    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n",
    "    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n",
    "    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
    "    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n",
    "    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n",
    "    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n",
    "    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n",
    "    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n",
    "    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n",
    "    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n",
    "    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n",
    "    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n",
    "    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n",
    "    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n",
    "    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n",
    "    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n",
    "    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n",
    "    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n",
    "    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n",
    "    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n",
    "    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n",
    "    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n",
    "    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n",
    "    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n",
    "    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n",
    "    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n",
    "    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n",
    "    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n",
    "    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n",
    "    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n",
    "    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n",
    "    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n",
    "    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n",
    "    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n",
    "    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n",
    "    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n",
    "    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n",
    "    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n",
    "    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n",
    "    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n",
    "    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n",
    "    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n",
    "    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n",
    "    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n",
    "    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n",
    "    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n",
    "    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n",
    "    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n",
    "    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n",
    "    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n",
    "    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n",
    "    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n",
    "    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n",
    "    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
    "    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n",
    "    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n",
    "    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n",
    "    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n",
    "    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n",
    "    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n",
    "    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n",
    "    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n",
    "    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n",
    "    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n",
    "    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n",
    "    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n",
    "    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n",
    "    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n",
    "    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n",
    "    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n",
    "    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n",
    "    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n",
    "    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
    "    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
    "    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n",
    "    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n",
    "    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n",
    "    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n",
    "    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n",
    "    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n",
    "    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n",
    "    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n",
    "    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n",
    "    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n",
    "    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n",
    "    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n",
    "    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n",
    "    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n",
    "    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n",
    "    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n",
    "    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n",
    "    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n",
    "    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n",
    "    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n",
    "    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n",
    "    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n",
    "    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n",
    "    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n",
    "    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n",
    "    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n",
    "    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n",
    "    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n",
    "    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n",
    "    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n",
    "    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n",
    "    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n",
    "    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n",
    "    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n",
    "    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n",
    "    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n",
    "    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n",
    "    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n",
    "    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n",
    "    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n",
    "    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n",
    "    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n",
    "    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n",
    "    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n",
    "    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n",
    "    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n",
    "    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n",
    "    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n",
    "    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n",
    "    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n",
    "    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n",
    "    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n",
    "    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n",
    "    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n",
    "    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n",
    "    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n",
    "    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n",
    "    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n",
    "    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n",
    "    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n",
    "    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n",
    "    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n",
    "    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n",
    "    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n",
    "    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n",
    "    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n",
    "    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n",
    "    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n",
    "    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n",
    "    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n",
    "    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n",
    "    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n",
    "    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n",
    "    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n",
    "    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n",
    "    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n",
    "    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n",
    "    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n",
    "    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n",
    "    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n",
    "    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n",
    "    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n",
    "    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n",
    "    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n",
    "    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n",
    "    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n",
    "    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n",
    "    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n",
    "    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
    "    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n",
    "    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n",
    "    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n",
    "    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n",
    "    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n",
    "    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n",
    "    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n",
    "    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n",
    "    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n",
    "    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n",
    "    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n",
    "    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n",
    "    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n",
    "    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n",
    "    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n",
    "    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n",
    "    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n",
    "    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n",
    "    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n",
    "    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n",
    "    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n",
    "    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n",
    "    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n",
    "    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n",
    "    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n",
    "    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n",
    "    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n",
    "    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n",
    "    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n",
    "    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
    "    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n",
    "    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n",
    "    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n",
    "    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n",
    "    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n",
    "    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n",
    "    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n",
    "    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n",
    "    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n",
    "    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n",
    "    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n",
    "    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n",
    "    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n",
    "    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n",
    "    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n",
    "    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n",
    "    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n",
    "    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n",
    "    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n",
    "    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n",
    "    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n",
    "    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n",
    "    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n",
    "    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n",
    "    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n",
    "    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n",
    "    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n",
    "    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n",
    "    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n",
    "    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n",
    "    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n",
    "    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n",
    "    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n",
    "    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n",
    "    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n",
    "    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n",
    "    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n",
    "    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n",
    "    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n",
    "    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n",
    "    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n",
    "    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n",
    "    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n",
    "           \n",
    "    # Urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        tweet = tweet.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ')      \n",
    "        \n",
    "    # Acronyms\n",
    "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
    "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
    "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
    "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
    "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
    "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
    "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
    "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
    "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n",
    "    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n",
    "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n",
    "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
    "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n",
    "    \n",
    "    # Grouping same words without embeddings\n",
    "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
    "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/x007ynb92t57yt7s517ct53w0000gn/T/ipykernel_7810/3037137197.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_all = data_sample.append(data_1_2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>Cuban doctors treat Mexico’s earthquake victims</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Twelve feared killed in Pakistani air ambulanc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>A  # pacena on her quest to help Mexico CIty a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>Big surprise from the party that has been usin...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>I just heard about the tsunami alert in New Ze...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "4540    Cuban doctors treat Mexico’s earthquake victims     0.0\n",
       "375   Twelve feared killed in Pakistani air ambulanc...     0.0\n",
       "4450  A  # pacena on her quest to help Mexico CIty a...     0.0\n",
       "3779  Big surprise from the party that has been usin...     0.0\n",
       "2485  I just heard about the tsunami alert in New Ze...     0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"all_labeled_tweet2.csv\")\n",
    "data = data[[\"text\",\"target\"]]\n",
    "data_1_2 = data.loc[(data['target'] == 1) | (data['target'] == 2)]\n",
    "data_0_3 = data.loc[(data['target'] == 0) | (data['target'] == 3)]\n",
    "data_sample = data_0_3.sample(n = 200)\n",
    "data_all = data_sample.append(data_1_2)\n",
    "data_all[\"text\"] = data_all[\"text\"].apply(str)\n",
    "data_all['text'] = data_all['text'].apply(lambda s : clean(s))\n",
    "data = data_all\n",
    "data.to_csv(\"sample_tweet_lab0123.csv\")\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Ignore</th>\n",
       "      <th>Machine_alert</th>\n",
       "      <th>Human_alert</th>\n",
       "      <th>Damages_alert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4540</th>\n",
       "      <td>Cuban doctors treat Mexico’s earthquake victims</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Twelve feared killed in Pakistani air ambulanc...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>A  # pacena on her quest to help Mexico CIty a...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>Big surprise from the party that has been usin...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>I just heard about the tsunami alert in New Ze...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  Ignore  \\\n",
       "4540    Cuban doctors treat Mexico’s earthquake victims    True   \n",
       "375   Twelve feared killed in Pakistani air ambulanc...    True   \n",
       "4450  A  # pacena on her quest to help Mexico CIty a...    True   \n",
       "3779  Big surprise from the party that has been usin...    True   \n",
       "2485  I just heard about the tsunami alert in New Ze...    True   \n",
       "\n",
       "      Machine_alert  Human_alert  Damages_alert  \n",
       "4540          False        False          False  \n",
       "375           False        False          False  \n",
       "4450          False        False          False  \n",
       "3779          False        False          False  \n",
       "2485          False        False          False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= pd.get_dummies(data.target, prefix='Label')\n",
    "dataset_dummies = pd.concat([data[\"text\"], y], axis=1)\n",
    "dataset_dummies[[\"Label_0.0\", \"Label_1.0\", \"Label_2.0\",\"Label_3.0\"]] = dataset_dummies[[\"Label_0.0\", \"Label_1.0\", \"Label_2.0\",\"Label_3.0\"]].astype(bool)\n",
    "dataset_dummies[\"Ignore\"] = dataset_dummies[\"Label_0.0\"]\n",
    "dataset_dummies[\"Machine_alert\"] = dataset_dummies[\"Label_1.0\"]\n",
    "dataset_dummies[\"Human_alert\"] = dataset_dummies[\"Label_2.0\"]\n",
    "dataset_dummies[\"Damages_alert\"] = dataset_dummies[\"Label_3.0\"]\n",
    "del dataset_dummies[\"Label_0.0\"]\n",
    "del dataset_dummies[\"Label_1.0\"]\n",
    "del dataset_dummies[\"Label_2.0\"]\n",
    "del dataset_dummies[\"Label_3.0\"]\n",
    "dataset_dummies.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder + define model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ae0c40cdef41fe8787c71c84ce378b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa8f874badd48c7bf3a1ff6b8bbb32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 203\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 88\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load BERTweet pre-trained model for embedding and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\")\n",
    "# model_embedding = AutoModel.from_pretrained(\"vinai/bertweet-large\")\n",
    "\n",
    "#Split dataset in train/test and convert from dataframe to dataset (Trasnformer library)\n",
    "train_data, test_data = train_test_split(dataset_dummies, test_size=0.3, random_state=42)\n",
    "dataset_train = Dataset.from_pandas(train_data)\n",
    "dataset_train = dataset_train.remove_columns(\"__index_level_0__\")\n",
    "dataset_test = Dataset.from_pandas(test_data)\n",
    "dataset_test = dataset_test.remove_columns(\"__index_level_0__\")\n",
    "\n",
    "dd = datasets.DatasetDict({\"train\":dataset_train,\"test\":dataset_test})\n",
    "\n",
    "#Prepare labels\n",
    "labels = [label for label in dd['train'].features.keys() if label not in ['text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"text\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # add labels\n",
    "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels_matrix = np.zeros((len(text), len(labels)))\n",
    "  # fill numpy array\n",
    "  for idx, label in enumerate(labels):\n",
    "    labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "  encoding[\"labels\"] = labels_matrix.tolist()\n",
    "  \n",
    "  return encoding\n",
    "\n",
    "encoded_dataset = dd.map(preprocess_data, batched=True, remove_columns=dd['train'].column_names)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-large\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"vinai/bertweet-large\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute_metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = 1\n",
    "# sigmoid = torch.nn.Sigmoid()\n",
    "# probs = sigmoid(torch.Tensor(predictions))\n",
    "# # next, use threshold to turn them into integer predictions\n",
    "# y_pred = np.zeros(probs.shape)\n",
    "# y_pred[probs.argmax()] = 1\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    # y_pred = np.zeros(probs.shape)\n",
    "    # # y_pred[probs.argmax()]\n",
    "    # y_pred[probs.argmax()] = 1\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6145, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-0.3152, -0.0789, -0.1917, -0.2529]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward pass\n",
    "outputs = model(\n",
    "    input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), \n",
    "    labels=encoded_dataset['train'][0]['labels'].unsqueeze(0)\n",
    ")\n",
    "outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 203\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 130\n",
      "  Number of trainable parameters = 355363844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940f85b299054a67b63e77d5416fcefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f14c4e990c4286b2f36364b583ffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vinai/bertweet-large/checkpoint-26\n",
      "Configuration saved in vinai/bertweet-large/checkpoint-26/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47413933277130127, 'eval_f1': 0.6091954022988506, 'eval_roc_auc': 0.7386363636363636, 'eval_accuracy': 0.5909090909090909, 'eval_runtime': 32.6549, 'eval_samples_per_second': 2.695, 'eval_steps_per_second': 0.337, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vinai/bertweet-large/checkpoint-26/pytorch_model.bin\n",
      "tokenizer config file saved in vinai/bertweet-large/checkpoint-26/tokenizer_config.json\n",
      "Special tokens file saved in vinai/bertweet-large/checkpoint-26/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cb9788a9ae40d39fed5309e4bd7371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vinai/bertweet-large/checkpoint-52\n",
      "Configuration saved in vinai/bertweet-large/checkpoint-52/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3627255856990814, 'eval_f1': 0.6625766871165645, 'eval_roc_auc': 0.7670454545454545, 'eval_accuracy': 0.6136363636363636, 'eval_runtime': 32.8759, 'eval_samples_per_second': 2.677, 'eval_steps_per_second': 0.335, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vinai/bertweet-large/checkpoint-52/pytorch_model.bin\n",
      "tokenizer config file saved in vinai/bertweet-large/checkpoint-52/tokenizer_config.json\n",
      "Special tokens file saved in vinai/bertweet-large/checkpoint-52/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb2e08ddb65478094a7516bf87548d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vinai/bertweet-large/checkpoint-78\n",
      "Configuration saved in vinai/bertweet-large/checkpoint-78/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26742038130760193, 'eval_f1': 0.7865168539325843, 'eval_roc_auc': 0.8598484848484848, 'eval_accuracy': 0.7727272727272727, 'eval_runtime': 31.8184, 'eval_samples_per_second': 2.766, 'eval_steps_per_second': 0.346, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vinai/bertweet-large/checkpoint-78/pytorch_model.bin\n",
      "tokenizer config file saved in vinai/bertweet-large/checkpoint-78/tokenizer_config.json\n",
      "Special tokens file saved in vinai/bertweet-large/checkpoint-78/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101dc21027634267a12bcd3ce475ef8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vinai/bertweet-large/checkpoint-104\n",
      "Configuration saved in vinai/bertweet-large/checkpoint-104/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22726693749427795, 'eval_f1': 0.8121212121212121, 'eval_roc_auc': 0.8617424242424242, 'eval_accuracy': 0.7613636363636364, 'eval_runtime': 27.312, 'eval_samples_per_second': 3.222, 'eval_steps_per_second': 0.403, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vinai/bertweet-large/checkpoint-104/pytorch_model.bin\n",
      "tokenizer config file saved in vinai/bertweet-large/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in vinai/bertweet-large/checkpoint-104/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee1d3c0589d4d05a2f219f09a823346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vinai/bertweet-large/checkpoint-130\n",
      "Configuration saved in vinai/bertweet-large/checkpoint-130/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21965736150741577, 'eval_f1': 0.8352941176470587, 'eval_roc_auc': 0.8825757575757576, 'eval_accuracy': 0.7954545454545454, 'eval_runtime': 26.9157, 'eval_samples_per_second': 3.269, 'eval_steps_per_second': 0.409, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vinai/bertweet-large/checkpoint-130/pytorch_model.bin\n",
      "tokenizer config file saved in vinai/bertweet-large/checkpoint-130/tokenizer_config.json\n",
      "Special tokens file saved in vinai/bertweet-large/checkpoint-130/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vinai/bertweet-large/checkpoint-130 (score: 0.8352941176470587).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1596.8238, 'train_samples_per_second': 0.636, 'train_steps_per_second': 0.081, 'train_loss': 0.2750523200401893, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=0.2750523200401893, metrics={'train_runtime': 1596.8238, 'train_samples_per_second': 0.636, 'train_steps_per_second': 0.081, 'train_loss': 0.2750523200401893, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to mymodel3\n",
      "Configuration saved in mymodel3/config.json\n",
      "Model weights saved in mymodel3/pytorch_model.bin\n",
      "tokenizer config file saved in mymodel3/tokenizer_config.json\n",
      "Special tokens file saved in mymodel3/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"mymodel3\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40430b16d62d4cf3b7330f07c880d261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.21965736150741577,\n",
       " 'eval_f1': 0.8352941176470587,\n",
       " 'eval_roc_auc': 0.8825757575757576,\n",
       " 'eval_accuracy': 0.7954545454545454,\n",
       " 'eval_runtime': 26.8346,\n",
       " 'eval_samples_per_second': 3.279,\n",
       " 'eval_steps_per_second': 0.41,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model pipeline\n",
    "tweet_label = []\n",
    "tweet_text = []\n",
    "\n",
    "def predict_tweet(x):\n",
    "    encoding = tokenizer(x, return_tensors=\"pt\")\n",
    "    encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}\n",
    "    outputs = trainer.model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    return labels[probs.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset to predict\n",
    "list_tweets = pd.read_csv(\"sample_tweet_lab0123.csv\")\n",
    "list_tweets = list_tweets[\"text\"].to_list()\n",
    "\n",
    "#Predict\n",
    "for x in list_tweets :\n",
    "    tweet_label.append(predict_tweet(x))\n",
    "    tweet_text.append(x)\n",
    "df_tweets_label_predicted = pd.DataFrame({'tweet_text': tweet_text, 'tweet_label':  tweet_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_label_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuban doctors treat Mexico’s earthquake victims</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twelve feared killed in Pakistani air ambulanc...</td>\n",
       "      <td>Damages_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A  # pacena on her quest to help Mexico CIty a...</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big surprise from the party that has been usin...</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I just heard about the tsunami alert in New Ze...</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text    tweet_label\n",
       "0    Cuban doctors treat Mexico’s earthquake victims         Ignore\n",
       "1  Twelve feared killed in Pakistani air ambulanc...  Damages_alert\n",
       "2  A  # pacena on her quest to help Mexico CIty a...         Ignore\n",
       "3  Big surprise from the party that has been usin...         Ignore\n",
       "4  I just heard about the tsunami alert in New Ze...         Ignore"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_label_predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ignore           152\n",
       "Machine_alert     67\n",
       "Damages_alert     66\n",
       "Human_alert        6\n",
       "Name: tweet_label, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_label_predicted[\"tweet_label\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING WITH SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ignore           2\n",
       "Machine_alert    1\n",
       "Name: tweet_label, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_label_try = []\n",
    "tweet_text_try = []\n",
    "list_tweets_try = [\"there is a earthquake happening Magnitude 3.4\", \"the ground is shaking\", \"my name is Olivier and happy to follow the Becode Programme #AI\"]\n",
    "for x_try in tqdm.tqdm(list_tweets_try):\n",
    "    predicted_lab_try = predict_tweet(x_try)\n",
    "    tweet_label_try.append(predicted_lab_try)\n",
    "    tweet_text_try.append(x_try)\n",
    "df_tweets_labeled_try = pd.DataFrame({'tweet_text': tweet_text_try, 'tweet_label':  tweet_label_try})\n",
    "df_tweets_labeled_try[\"tweet_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model on productiond dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from azure.cosmos import CosmosClient, ContainerProxy, DatabaseProxy, PartitionKey\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get data from Cosmos db\n",
    "# azure-core==1.26.2\n",
    "# pip install azure-core\n",
    "# azure-cosmos==4.3.0\n",
    "# pip install azure-cosmos\n",
    "\n",
    "endpoint = \"https://bouman-earthquake-db.documents.azure.com:443/\"\n",
    "key = \"kqshZbgJfMqCfLMaojjtNkXoYzKhe2BaaPgcrlDIdURoW1dWVPJkH9nsTXO2c1eaKXmQg20MwMSbACDb8XLizQ==\"\n",
    "def connect(endpoint:str, key:str) -> ContainerProxy:\n",
    "    client = CosmosClient(endpoint, key)\n",
    "    database_name = \"raw_tweets\"\n",
    "    container_name = \"tweets_db\"\n",
    "    partition_key = PartitionKey(path=\"/id\")\n",
    "    database:DatabaseProxy = client.create_database_if_not_exists(id=database_name)\n",
    "    container:ContainerProxy = database.create_container_if_not_exists(id=container_name, partition_key=partition_key)\n",
    "    return container\n",
    "\n",
    "def query_cosmosdb(query:str):\n",
    "    container = connect(endpoint=endpoint, key=key)\n",
    "    items = list(container.query_items(\n",
    "        query=query,\n",
    "        enable_cross_partition_query=True\n",
    "    ))\n",
    "    return items \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conection to cosmos DB\n",
    "raw_tweets = query_cosmosdb(\"SELECT * FROM r\")  #Select all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate dunction\n",
    "def translate_tweet_f(dic:dict)-> dict :\n",
    "    '''function that take dic and add a new colums who translate the text to english'''\n",
    "    try:\n",
    "        x = GoogleTranslator(source='auto', target='en').translate(dic['tweet_text'])\n",
    "        dic['translate_text'] = x        \n",
    "    except KeyError as err:\n",
    "        dic['translate_text'] = ''\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_tweet in raw_tweets:\n",
    "    #transform in english \n",
    "    raw_tweet = translate_tweet_f(raw_tweet)\n",
    "    eng_text = raw_tweet['translate_text']\n",
    "    label = predict_tweet(eng_text)\n",
    "    #update dictionary\n",
    "    raw_tweet[\"ml_label\"]= label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_tweets\n",
    "json_object = json.dumps(raw_tweets, indent=4)\n",
    "with open(\"sample.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_author_id</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>tweet_lang</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_username</th>\n",
       "      <th>place_country</th>\n",
       "      <th>place_city</th>\n",
       "      <th>place_geo</th>\n",
       "      <th>place_place_type</th>\n",
       "      <th>_rid</th>\n",
       "      <th>_self</th>\n",
       "      <th>_etag</th>\n",
       "      <th>_attachments</th>\n",
       "      <th>ml_processed</th>\n",
       "      <th>translate_text</th>\n",
       "      <th>_ts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ml_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Damages_alert</th>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "      <td>1855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human_alert</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ignore</th>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "      <td>4857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Machine_alert</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  tweet_text  tweet_author_id  tweet_created_at  \\\n",
       "ml_label                                                             \n",
       "Damages_alert  1855        1855             1855              1855   \n",
       "Human_alert       6           6                6                 6   \n",
       "Ignore         4857        4857             4857              4857   \n",
       "Machine_alert  1503        1503             1503              1503   \n",
       "\n",
       "               tweet_lang  user_id  user_username  place_country  place_city  \\\n",
       "ml_label                                                                       \n",
       "Damages_alert        1855     1855           1855              0           0   \n",
       "Human_alert             6        6              6              0           0   \n",
       "Ignore               4857     4857           4857              2           2   \n",
       "Machine_alert        1503     1503           1503              4           4   \n",
       "\n",
       "               place_geo  place_place_type  _rid  _self  _etag  _attachments  \\\n",
       "ml_label                                                                       \n",
       "Damages_alert          0                 0  1855   1855   1855          1855   \n",
       "Human_alert            0                 0     6      6      6             6   \n",
       "Ignore                 1                 1  4857   4857   4857          4857   \n",
       "Machine_alert          3                 3  1503   1503   1503          1503   \n",
       "\n",
       "               ml_processed  translate_text   _ts  \n",
       "ml_label                                           \n",
       "Damages_alert          1855            1855  1855  \n",
       "Human_alert               6               6     6  \n",
       "Ignore                 4857            4857  4857  \n",
       "Machine_alert          1503            1503  1503  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('sample.json')\n",
    "df.head()\n",
    "df.groupby(by=[\"ml_label\"], dropna=False).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8221, 19)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_author_id</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>tweet_lang</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_username</th>\n",
       "      <th>place_country</th>\n",
       "      <th>place_city</th>\n",
       "      <th>place_geo</th>\n",
       "      <th>place_place_type</th>\n",
       "      <th>_rid</th>\n",
       "      <th>_self</th>\n",
       "      <th>_etag</th>\n",
       "      <th>_attachments</th>\n",
       "      <th>ml_processed</th>\n",
       "      <th>translate_text</th>\n",
       "      <th>_ts</th>\n",
       "      <th>ml_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1617465854827831296</td>\n",
       "      <td>@terremoto_44 Exactly. If you’re making it abo...</td>\n",
       "      <td>794618296956030976</td>\n",
       "      <td>2023-01-23 10:14:45+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>794618296956030976</td>\n",
       "      <td>FrankTheBeef5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwBAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200f546-0000-0d00-0000-63d147420000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@terremoto_44 Exactly. If you’re making it abo...</td>\n",
       "      <td>1674659650</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1617466179022426112</td>\n",
       "      <td>RT @URDailyHistory: 23 Jan 1556: What is thoug...</td>\n",
       "      <td>704934261414109184</td>\n",
       "      <td>2023-01-23 10:16:02+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>704934261414109184</td>\n",
       "      <td>1nmemoriam</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwCAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200f646-0000-0d00-0000-63d147430000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>RT @URDailyHistory: 23 Jan 1556: What is thoug...</td>\n",
       "      <td>1674659651</td>\n",
       "      <td>Damages_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1617466449253269504</td>\n",
       "      <td>RT @anc_party: Many wonder why H.E @MusaliaMud...</td>\n",
       "      <td>1570368522517102592</td>\n",
       "      <td>2023-01-23 10:17:07+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1570368522517102592</td>\n",
       "      <td>konzolo_ma</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwDAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200f746-0000-0d00-0000-63d147440000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>RT @anc_party: Many wonder why H.E @MusaliaMud...</td>\n",
       "      <td>1674659652</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1617466838576926720</td>\n",
       "      <td>@BIO99_BIO99 Can someone explain what this mea...</td>\n",
       "      <td>1224973611452313600</td>\n",
       "      <td>2023-01-23 10:18:40+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1224973611452313600</td>\n",
       "      <td>Tobias18910911</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwEAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200fa46-0000-0d00-0000-63d147440000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@BIO99_BIO99 Can someone explain what this mea...</td>\n",
       "      <td>1674659652</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1617468180842123264</td>\n",
       "      <td>@DaisyKenyan_ Walidhani deep state watafanya I...</td>\n",
       "      <td>2747335167</td>\n",
       "      <td>2023-01-23 10:24:00+00:00</td>\n",
       "      <td>hi</td>\n",
       "      <td>2747335167</td>\n",
       "      <td>njengapeter160</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwFAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200fb46-0000-0d00-0000-63d147450000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@DaisyKenyan_ They thought the deep state woul...</td>\n",
       "      <td>1674659653</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1617469692096176128</td>\n",
       "      <td>LaCie 130813 USB 2 0 PCI Card Design by Sismo ...</td>\n",
       "      <td>1581842757718417408</td>\n",
       "      <td>2023-01-23 10:30:00+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1581842757718417408</td>\n",
       "      <td>camron59c</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwGAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200fc46-0000-0d00-0000-63d147460000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>LaCie 130813 USB 2 0 PCI Card Design by Sismo ...</td>\n",
       "      <td>1674659654</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1617470160260005888</td>\n",
       "      <td>@Dr_Blazquez Contento / Tetero / Teatro / Tont...</td>\n",
       "      <td>177903913</td>\n",
       "      <td>2023-01-23 10:31:52+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>177903913</td>\n",
       "      <td>femadi66</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwHAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200fd46-0000-0d00-0000-63d147490000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@Dr_Blazquez Contentment / Teapot / Theater / ...</td>\n",
       "      <td>1674659657</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1617471380961779712</td>\n",
       "      <td>【ML3.2】AUSTRALIA OCCIDENTAL, AUSTRLIA 10km 23/...</td>\n",
       "      <td>2920049156</td>\n",
       "      <td>2023-01-23 10:36:43+00:00</td>\n",
       "      <td>ro</td>\n",
       "      <td>2920049156</td>\n",
       "      <td>eq_map_es2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwIAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200fe46-0000-0d00-0000-63d1474a0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>【ML3.2】WESTERN AUSTRALIA, AUSTRALIA 10km 23/01...</td>\n",
       "      <td>1674659658</td>\n",
       "      <td>Machine_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1617471844940152832</td>\n",
       "      <td>Automatically Plotted Shakemaps:\\n* 2023-01-23...</td>\n",
       "      <td>1310861189425934336</td>\n",
       "      <td>2023-01-23 10:38:33+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1310861189425934336</td>\n",
       "      <td>eew_p</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwJAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"0200ff46-0000-0d00-0000-63d1474b0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>Automatically Plotted Shakemaps:\\n* 2023-01-23...</td>\n",
       "      <td>1674659659</td>\n",
       "      <td>Machine_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1617472253431582720</td>\n",
       "      <td>#Earthquake (#terremot) possibly felt 44 sec a...</td>\n",
       "      <td>203226820</td>\n",
       "      <td>2023-01-23 10:40:11+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>203226820</td>\n",
       "      <td>LastQuake</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwKAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000047-0000-0d00-0000-63d1474c0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>#Earthquake (#terremot) possibly felt 44 sec a...</td>\n",
       "      <td>1674659660</td>\n",
       "      <td>Machine_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1617472363934912512</td>\n",
       "      <td>@nwdiadema @mumusho111 @VitalikButerin @Please...</td>\n",
       "      <td>1692021396</td>\n",
       "      <td>2023-01-23 10:40:37+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1692021396</td>\n",
       "      <td>luchytori</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwLAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000147-0000-0d00-0000-63d1474d0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@nwdiadema @mumusho111 @VitalikButerin @Please...</td>\n",
       "      <td>1674659661</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1617472406439723008</td>\n",
       "      <td>@Hakanawzyl @visegrad24 What you on about bloke?</td>\n",
       "      <td>223782828</td>\n",
       "      <td>2023-01-23 10:40:47+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>223782828</td>\n",
       "      <td>unhappynow</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwMAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000247-0000-0d00-0000-63d1474e0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@Hakanawzyl @visegrad24 What you on about bloke?</td>\n",
       "      <td>1674659662</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1617472599121870848</td>\n",
       "      <td>@mustafahamed28 @abigaildenila @mumusho111 @Vi...</td>\n",
       "      <td>1440204319</td>\n",
       "      <td>2023-01-23 10:41:33+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1440204319</td>\n",
       "      <td>UtaricandraD</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwNAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000347-0000-0d00-0000-63d147500000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@mustafahamed28 @abigaildenila @mumusho111 @Vi...</td>\n",
       "      <td>1674659664</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1617472649638326272</td>\n",
       "      <td>@abhieomosura @abigaildenila @hudlowjon1 @Vita...</td>\n",
       "      <td>1401341515</td>\n",
       "      <td>2023-01-23 10:41:45+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1401341515</td>\n",
       "      <td>kingtalha786</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwOAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000447-0000-0d00-0000-63d147510000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@abhieomosura @abigaildenila @hudlowjon1 @Vita...</td>\n",
       "      <td>1674659665</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1617473477468098560</td>\n",
       "      <td>@Biancawamu2 https://t.co/WFDpgdsp7A</td>\n",
       "      <td>1297908245655367680</td>\n",
       "      <td>2023-01-23 10:45:02+00:00</td>\n",
       "      <td>qme</td>\n",
       "      <td>1297908245655367680</td>\n",
       "      <td>TonuiAlex</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwPAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000547-0000-0d00-0000-63d147520000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@Biancawamu2 https://t.co/WFDpgdsp7A</td>\n",
       "      <td>1674659666</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1617473736969519104</td>\n",
       "      <td>@LastQuake Yes felt it from St Paul’s bay malta</td>\n",
       "      <td>828991153</td>\n",
       "      <td>2023-01-23 10:46:04+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>828991153</td>\n",
       "      <td>Stefydi84</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwQAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000647-0000-0d00-0000-63d147530000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@LastQuake Yes felt it from St Paul’s bay malta</td>\n",
       "      <td>1674659667</td>\n",
       "      <td>Damages_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1617473900698546176</td>\n",
       "      <td>@byzantinepower For the Greeks the war was rea...</td>\n",
       "      <td>1295346380475596800</td>\n",
       "      <td>2023-01-23 10:46:43+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1295346380475596800</td>\n",
       "      <td>retsinokinitoc</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwRAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000747-0000-0d00-0000-63d147540000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@byzantinepower For the Greeks the war was rea...</td>\n",
       "      <td>1674659668</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1617474237593174016</td>\n",
       "      <td>USGS reports a M0.89 earthquake, 8km W of Cobb...</td>\n",
       "      <td>1414684496</td>\n",
       "      <td>2023-01-23 10:48:04+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1414684496</td>\n",
       "      <td>everyEarthquake</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwSAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000847-0000-0d00-0000-63d147550000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>USGS reports a M0.89 earthquake, 8km W of Cobb...</td>\n",
       "      <td>1674659669</td>\n",
       "      <td>Machine_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1617474581941329920</td>\n",
       "      <td>Update Info Earthquake 56.88 km of Stork Rock ...</td>\n",
       "      <td>1197989340984426496</td>\n",
       "      <td>2023-01-23 10:49:26+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1197989340984426496</td>\n",
       "      <td>VolcanoEWS</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwTAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000b47-0000-0d00-0000-63d147560000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>Update Info Earthquake 56.88 km of Stork Rock ...</td>\n",
       "      <td>1674659670</td>\n",
       "      <td>Machine_alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1617475280326754304</td>\n",
       "      <td>@yuezhii @amerhomedy @HERSHEYSNYC2010 @Marsels...</td>\n",
       "      <td>1440204319</td>\n",
       "      <td>2023-01-23 10:52:12+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1440204319</td>\n",
       "      <td>UtaricandraD</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwUAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000c47-0000-0d00-0000-63d147570000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@yuezhii @amerhomedy @HERSHEYSNYC2010 @Marsels...</td>\n",
       "      <td>1674659671</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1617475655549210624</td>\n",
       "      <td>@LPyex3 What happens if clouds rain Men 🤔\\n\\nW...</td>\n",
       "      <td>1489185241767972864</td>\n",
       "      <td>2023-01-23 10:53:42+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1489185241767972864</td>\n",
       "      <td>ZenDematos</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwVAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000d47-0000-0d00-0000-63d147580000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@LPyex3 What happens if clouds rain Men 🤔\\n\\nW...</td>\n",
       "      <td>1674659672</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1617475874936483840</td>\n",
       "      <td>@VitalikButerin @galovera3 @CoinDesk @Bitstamp...</td>\n",
       "      <td>1525204532</td>\n",
       "      <td>2023-01-23 10:54:34+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1525204532</td>\n",
       "      <td>wesamakram2014</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwWAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000e47-0000-0d00-0000-63d147590000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@VitalikButerin @galovera3 @CoinDesk @Bitstamp...</td>\n",
       "      <td>1674659673</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1617475958885478400</td>\n",
       "      <td>@_antoniooo4 terremoto</td>\n",
       "      <td>1238422468358004736</td>\n",
       "      <td>2023-01-23 10:54:54+00:00</td>\n",
       "      <td>es</td>\n",
       "      <td>1238422468358004736</td>\n",
       "      <td>carlaaponce_</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwXAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02000f47-0000-0d00-0000-63d1475a0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@_antoniooo4 earthquake</td>\n",
       "      <td>1674659674</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1617476124212355072</td>\n",
       "      <td>#poweroutage #Rawalpindi #Lahore #Peshawar #Su...</td>\n",
       "      <td>1610090917213474816</td>\n",
       "      <td>2023-01-23 10:55:33+00:00</td>\n",
       "      <td>qht</td>\n",
       "      <td>1610090917213474816</td>\n",
       "      <td>Ibrahimshah23</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwYAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02001047-0000-0d00-0000-63d1475b0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>#poweroutage #Rawalpindi #Lahore #Peshawar #Su...</td>\n",
       "      <td>1674659675</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1617476243242483712</td>\n",
       "      <td>@wesamakram2014 @VitalikButerin @galovera3 @Co...</td>\n",
       "      <td>1711534058</td>\n",
       "      <td>2023-01-23 10:56:02+00:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1711534058</td>\n",
       "      <td>ehnarsky</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LaN4AIJYJhwZAAAAAAAAAA==</td>\n",
       "      <td>dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...</td>\n",
       "      <td>\"02001147-0000-0d00-0000-63d1475d0000\"</td>\n",
       "      <td>attachments/</td>\n",
       "      <td>True</td>\n",
       "      <td>@wesamakram2014 @VitalikButerin @galovera3 @Co...</td>\n",
       "      <td>1674659677</td>\n",
       "      <td>Ignore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                         tweet_text  \\\n",
       "0   1617465854827831296  @terremoto_44 Exactly. If you’re making it abo...   \n",
       "1   1617466179022426112  RT @URDailyHistory: 23 Jan 1556: What is thoug...   \n",
       "2   1617466449253269504  RT @anc_party: Many wonder why H.E @MusaliaMud...   \n",
       "3   1617466838576926720  @BIO99_BIO99 Can someone explain what this mea...   \n",
       "4   1617468180842123264  @DaisyKenyan_ Walidhani deep state watafanya I...   \n",
       "5   1617469692096176128  LaCie 130813 USB 2 0 PCI Card Design by Sismo ...   \n",
       "6   1617470160260005888  @Dr_Blazquez Contento / Tetero / Teatro / Tont...   \n",
       "7   1617471380961779712  【ML3.2】AUSTRALIA OCCIDENTAL, AUSTRLIA 10km 23/...   \n",
       "8   1617471844940152832  Automatically Plotted Shakemaps:\\n* 2023-01-23...   \n",
       "9   1617472253431582720  #Earthquake (#terremot) possibly felt 44 sec a...   \n",
       "10  1617472363934912512  @nwdiadema @mumusho111 @VitalikButerin @Please...   \n",
       "11  1617472406439723008   @Hakanawzyl @visegrad24 What you on about bloke?   \n",
       "12  1617472599121870848  @mustafahamed28 @abigaildenila @mumusho111 @Vi...   \n",
       "13  1617472649638326272  @abhieomosura @abigaildenila @hudlowjon1 @Vita...   \n",
       "14  1617473477468098560               @Biancawamu2 https://t.co/WFDpgdsp7A   \n",
       "15  1617473736969519104    @LastQuake Yes felt it from St Paul’s bay malta   \n",
       "16  1617473900698546176  @byzantinepower For the Greeks the war was rea...   \n",
       "17  1617474237593174016  USGS reports a M0.89 earthquake, 8km W of Cobb...   \n",
       "18  1617474581941329920  Update Info Earthquake 56.88 km of Stork Rock ...   \n",
       "19  1617475280326754304  @yuezhii @amerhomedy @HERSHEYSNYC2010 @Marsels...   \n",
       "20  1617475655549210624  @LPyex3 What happens if clouds rain Men 🤔\\n\\nW...   \n",
       "21  1617475874936483840  @VitalikButerin @galovera3 @CoinDesk @Bitstamp...   \n",
       "22  1617475958885478400                             @_antoniooo4 terremoto   \n",
       "23  1617476124212355072  #poweroutage #Rawalpindi #Lahore #Peshawar #Su...   \n",
       "24  1617476243242483712  @wesamakram2014 @VitalikButerin @galovera3 @Co...   \n",
       "\n",
       "        tweet_author_id          tweet_created_at tweet_lang  \\\n",
       "0    794618296956030976 2023-01-23 10:14:45+00:00         en   \n",
       "1    704934261414109184 2023-01-23 10:16:02+00:00         en   \n",
       "2   1570368522517102592 2023-01-23 10:17:07+00:00         en   \n",
       "3   1224973611452313600 2023-01-23 10:18:40+00:00         en   \n",
       "4            2747335167 2023-01-23 10:24:00+00:00         hi   \n",
       "5   1581842757718417408 2023-01-23 10:30:00+00:00         es   \n",
       "6             177903913 2023-01-23 10:31:52+00:00         es   \n",
       "7            2920049156 2023-01-23 10:36:43+00:00         ro   \n",
       "8   1310861189425934336 2023-01-23 10:38:33+00:00         en   \n",
       "9             203226820 2023-01-23 10:40:11+00:00         en   \n",
       "10           1692021396 2023-01-23 10:40:37+00:00         en   \n",
       "11            223782828 2023-01-23 10:40:47+00:00         en   \n",
       "12           1440204319 2023-01-23 10:41:33+00:00         en   \n",
       "13           1401341515 2023-01-23 10:41:45+00:00         en   \n",
       "14  1297908245655367680 2023-01-23 10:45:02+00:00        qme   \n",
       "15            828991153 2023-01-23 10:46:04+00:00         en   \n",
       "16  1295346380475596800 2023-01-23 10:46:43+00:00         en   \n",
       "17           1414684496 2023-01-23 10:48:04+00:00         en   \n",
       "18  1197989340984426496 2023-01-23 10:49:26+00:00         en   \n",
       "19           1440204319 2023-01-23 10:52:12+00:00         en   \n",
       "20  1489185241767972864 2023-01-23 10:53:42+00:00         en   \n",
       "21           1525204532 2023-01-23 10:54:34+00:00         en   \n",
       "22  1238422468358004736 2023-01-23 10:54:54+00:00         es   \n",
       "23  1610090917213474816 2023-01-23 10:55:33+00:00        qht   \n",
       "24           1711534058 2023-01-23 10:56:02+00:00         en   \n",
       "\n",
       "                user_id    user_username place_country place_city place_geo  \\\n",
       "0    794618296956030976    FrankTheBeef5          None       None      None   \n",
       "1    704934261414109184       1nmemoriam          None       None      None   \n",
       "2   1570368522517102592       konzolo_ma          None       None      None   \n",
       "3   1224973611452313600   Tobias18910911          None       None      None   \n",
       "4            2747335167   njengapeter160          None       None      None   \n",
       "5   1581842757718417408        camron59c          None       None      None   \n",
       "6             177903913         femadi66          None       None      None   \n",
       "7            2920049156       eq_map_es2          None       None      None   \n",
       "8   1310861189425934336            eew_p          None       None      None   \n",
       "9             203226820        LastQuake          None       None      None   \n",
       "10           1692021396        luchytori          None       None      None   \n",
       "11            223782828       unhappynow          None       None      None   \n",
       "12           1440204319     UtaricandraD          None       None      None   \n",
       "13           1401341515     kingtalha786          None       None      None   \n",
       "14  1297908245655367680        TonuiAlex          None       None      None   \n",
       "15            828991153        Stefydi84          None       None      None   \n",
       "16  1295346380475596800   retsinokinitoc          None       None      None   \n",
       "17           1414684496  everyEarthquake          None       None      None   \n",
       "18  1197989340984426496       VolcanoEWS          None       None      None   \n",
       "19           1440204319     UtaricandraD          None       None      None   \n",
       "20  1489185241767972864       ZenDematos          None       None      None   \n",
       "21           1525204532   wesamakram2014          None       None      None   \n",
       "22  1238422468358004736     carlaaponce_          None       None      None   \n",
       "23  1610090917213474816    Ibrahimshah23          None       None      None   \n",
       "24           1711534058         ehnarsky          None       None      None   \n",
       "\n",
       "   place_place_type                      _rid  \\\n",
       "0              None  LaN4AIJYJhwBAAAAAAAAAA==   \n",
       "1              None  LaN4AIJYJhwCAAAAAAAAAA==   \n",
       "2              None  LaN4AIJYJhwDAAAAAAAAAA==   \n",
       "3              None  LaN4AIJYJhwEAAAAAAAAAA==   \n",
       "4              None  LaN4AIJYJhwFAAAAAAAAAA==   \n",
       "5              None  LaN4AIJYJhwGAAAAAAAAAA==   \n",
       "6              None  LaN4AIJYJhwHAAAAAAAAAA==   \n",
       "7              None  LaN4AIJYJhwIAAAAAAAAAA==   \n",
       "8              None  LaN4AIJYJhwJAAAAAAAAAA==   \n",
       "9              None  LaN4AIJYJhwKAAAAAAAAAA==   \n",
       "10             None  LaN4AIJYJhwLAAAAAAAAAA==   \n",
       "11             None  LaN4AIJYJhwMAAAAAAAAAA==   \n",
       "12             None  LaN4AIJYJhwNAAAAAAAAAA==   \n",
       "13             None  LaN4AIJYJhwOAAAAAAAAAA==   \n",
       "14             None  LaN4AIJYJhwPAAAAAAAAAA==   \n",
       "15             None  LaN4AIJYJhwQAAAAAAAAAA==   \n",
       "16             None  LaN4AIJYJhwRAAAAAAAAAA==   \n",
       "17             None  LaN4AIJYJhwSAAAAAAAAAA==   \n",
       "18             None  LaN4AIJYJhwTAAAAAAAAAA==   \n",
       "19             None  LaN4AIJYJhwUAAAAAAAAAA==   \n",
       "20             None  LaN4AIJYJhwVAAAAAAAAAA==   \n",
       "21             None  LaN4AIJYJhwWAAAAAAAAAA==   \n",
       "22             None  LaN4AIJYJhwXAAAAAAAAAA==   \n",
       "23             None  LaN4AIJYJhwYAAAAAAAAAA==   \n",
       "24             None  LaN4AIJYJhwZAAAAAAAAAA==   \n",
       "\n",
       "                                                _self  \\\n",
       "0   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "1   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "2   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "3   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "4   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "5   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "6   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "7   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "8   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "9   dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "10  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "11  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "12  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "13  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "14  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "15  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "16  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "17  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "18  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "19  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "20  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "21  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "22  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "23  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "24  dbs/LaN4AA==/colls/LaN4AIJYJhw=/docs/LaN4AIJYJ...   \n",
       "\n",
       "                                     _etag  _attachments  ml_processed  \\\n",
       "0   \"0200f546-0000-0d00-0000-63d147420000\"  attachments/          True   \n",
       "1   \"0200f646-0000-0d00-0000-63d147430000\"  attachments/          True   \n",
       "2   \"0200f746-0000-0d00-0000-63d147440000\"  attachments/          True   \n",
       "3   \"0200fa46-0000-0d00-0000-63d147440000\"  attachments/          True   \n",
       "4   \"0200fb46-0000-0d00-0000-63d147450000\"  attachments/          True   \n",
       "5   \"0200fc46-0000-0d00-0000-63d147460000\"  attachments/          True   \n",
       "6   \"0200fd46-0000-0d00-0000-63d147490000\"  attachments/          True   \n",
       "7   \"0200fe46-0000-0d00-0000-63d1474a0000\"  attachments/          True   \n",
       "8   \"0200ff46-0000-0d00-0000-63d1474b0000\"  attachments/          True   \n",
       "9   \"02000047-0000-0d00-0000-63d1474c0000\"  attachments/          True   \n",
       "10  \"02000147-0000-0d00-0000-63d1474d0000\"  attachments/          True   \n",
       "11  \"02000247-0000-0d00-0000-63d1474e0000\"  attachments/          True   \n",
       "12  \"02000347-0000-0d00-0000-63d147500000\"  attachments/          True   \n",
       "13  \"02000447-0000-0d00-0000-63d147510000\"  attachments/          True   \n",
       "14  \"02000547-0000-0d00-0000-63d147520000\"  attachments/          True   \n",
       "15  \"02000647-0000-0d00-0000-63d147530000\"  attachments/          True   \n",
       "16  \"02000747-0000-0d00-0000-63d147540000\"  attachments/          True   \n",
       "17  \"02000847-0000-0d00-0000-63d147550000\"  attachments/          True   \n",
       "18  \"02000b47-0000-0d00-0000-63d147560000\"  attachments/          True   \n",
       "19  \"02000c47-0000-0d00-0000-63d147570000\"  attachments/          True   \n",
       "20  \"02000d47-0000-0d00-0000-63d147580000\"  attachments/          True   \n",
       "21  \"02000e47-0000-0d00-0000-63d147590000\"  attachments/          True   \n",
       "22  \"02000f47-0000-0d00-0000-63d1475a0000\"  attachments/          True   \n",
       "23  \"02001047-0000-0d00-0000-63d1475b0000\"  attachments/          True   \n",
       "24  \"02001147-0000-0d00-0000-63d1475d0000\"  attachments/          True   \n",
       "\n",
       "                                       translate_text         _ts  \\\n",
       "0   @terremoto_44 Exactly. If you’re making it abo...  1674659650   \n",
       "1   RT @URDailyHistory: 23 Jan 1556: What is thoug...  1674659651   \n",
       "2   RT @anc_party: Many wonder why H.E @MusaliaMud...  1674659652   \n",
       "3   @BIO99_BIO99 Can someone explain what this mea...  1674659652   \n",
       "4   @DaisyKenyan_ They thought the deep state woul...  1674659653   \n",
       "5   LaCie 130813 USB 2 0 PCI Card Design by Sismo ...  1674659654   \n",
       "6   @Dr_Blazquez Contentment / Teapot / Theater / ...  1674659657   \n",
       "7   【ML3.2】WESTERN AUSTRALIA, AUSTRALIA 10km 23/01...  1674659658   \n",
       "8   Automatically Plotted Shakemaps:\\n* 2023-01-23...  1674659659   \n",
       "9   #Earthquake (#terremot) possibly felt 44 sec a...  1674659660   \n",
       "10  @nwdiadema @mumusho111 @VitalikButerin @Please...  1674659661   \n",
       "11   @Hakanawzyl @visegrad24 What you on about bloke?  1674659662   \n",
       "12  @mustafahamed28 @abigaildenila @mumusho111 @Vi...  1674659664   \n",
       "13  @abhieomosura @abigaildenila @hudlowjon1 @Vita...  1674659665   \n",
       "14               @Biancawamu2 https://t.co/WFDpgdsp7A  1674659666   \n",
       "15    @LastQuake Yes felt it from St Paul’s bay malta  1674659667   \n",
       "16  @byzantinepower For the Greeks the war was rea...  1674659668   \n",
       "17  USGS reports a M0.89 earthquake, 8km W of Cobb...  1674659669   \n",
       "18  Update Info Earthquake 56.88 km of Stork Rock ...  1674659670   \n",
       "19  @yuezhii @amerhomedy @HERSHEYSNYC2010 @Marsels...  1674659671   \n",
       "20  @LPyex3 What happens if clouds rain Men 🤔\\n\\nW...  1674659672   \n",
       "21  @VitalikButerin @galovera3 @CoinDesk @Bitstamp...  1674659673   \n",
       "22                            @_antoniooo4 earthquake  1674659674   \n",
       "23  #poweroutage #Rawalpindi #Lahore #Peshawar #Su...  1674659675   \n",
       "24  @wesamakram2014 @VitalikButerin @galovera3 @Co...  1674659677   \n",
       "\n",
       "         ml_label  \n",
       "0          Ignore  \n",
       "1   Damages_alert  \n",
       "2          Ignore  \n",
       "3          Ignore  \n",
       "4          Ignore  \n",
       "5          Ignore  \n",
       "6          Ignore  \n",
       "7   Machine_alert  \n",
       "8   Machine_alert  \n",
       "9   Machine_alert  \n",
       "10         Ignore  \n",
       "11         Ignore  \n",
       "12         Ignore  \n",
       "13         Ignore  \n",
       "14         Ignore  \n",
       "15  Damages_alert  \n",
       "16         Ignore  \n",
       "17  Machine_alert  \n",
       "18  Machine_alert  \n",
       "19         Ignore  \n",
       "20         Ignore  \n",
       "21         Ignore  \n",
       "22         Ignore  \n",
       "23         Ignore  \n",
       "24         Ignore  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
